{
    "cells": [
        {
            "metadata": {
                "collapsed": true
            },
            "cell_type": "markdown",
            "source": "![](https://pbs.twimg.com/profile_images/1240321823213907973/TFsX7hfq_400x400.jpg)\n<h1 align=center><font size = 5><span style=\"color:gray\">FINAL PROJECT DEEP LEARNING & NEURAL NETWORKS WITH KERAS</span></font></h1>\n\n### <span style=\"color:gray\">Project: *Build a regression model using the Deep learning <span style=\"color:lightblue\">Keras</span> library*</span>"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "## Table of Contents\n\n<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n\n<font size = 3>\n\n1. <a href=\"#itemA\">A. Build a baseline model</a>      \n2. <a href=\"#itemB\">B. Repeat Part A but use a normalized version of the data</a>     \n3. <a href=\"#itemC\">C. Increate the number of epochs</a>\n4. <a href=\"#itemD\">D. Increase the number of hidden layers</a>\n5. <a href=\"#itemE\">E. Additional</a>\n6. <a href=\"#itemF\">F. Report</a>\n\n</font>\n</div>"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "<a id='itemA'></a>\n### A. Build a baseline model "
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# First import the 'pandas' and 'numpy' libraries\nimport pandas as pd # 'pd' constructor\nimport numpy as np  # 'np' constructor",
            "execution_count": 874,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Import the Data Set\nconcrete_data=pd.read_csv('https://cocl.us/concrete_data')\n# Display First five (5) rows of data set\nconcrete_data.head()",
            "execution_count": 875,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 875,
                    "data": {
                        "text/plain": "   Cement  Blast Furnace Slag  Fly Ash  Water  Superplasticizer  \\\n0   540.0                 0.0      0.0  162.0               2.5   \n1   540.0                 0.0      0.0  162.0               2.5   \n2   332.5               142.5      0.0  228.0               0.0   \n3   332.5               142.5      0.0  228.0               0.0   \n4   198.6               132.4      0.0  192.0               0.0   \n\n   Coarse Aggregate  Fine Aggregate  Age  Strength  \n0            1040.0           676.0   28     79.99  \n1            1055.0           676.0   28     61.89  \n2             932.0           594.0  270     40.27  \n3             932.0           594.0  365     41.05  \n4             978.4           825.5  360     44.30  ",
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Cement</th>\n      <th>Blast Furnace Slag</th>\n      <th>Fly Ash</th>\n      <th>Water</th>\n      <th>Superplasticizer</th>\n      <th>Coarse Aggregate</th>\n      <th>Fine Aggregate</th>\n      <th>Age</th>\n      <th>Strength</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>540.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>162.0</td>\n      <td>2.5</td>\n      <td>1040.0</td>\n      <td>676.0</td>\n      <td>28</td>\n      <td>79.99</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>540.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>162.0</td>\n      <td>2.5</td>\n      <td>1055.0</td>\n      <td>676.0</td>\n      <td>28</td>\n      <td>61.89</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>332.5</td>\n      <td>142.5</td>\n      <td>0.0</td>\n      <td>228.0</td>\n      <td>0.0</td>\n      <td>932.0</td>\n      <td>594.0</td>\n      <td>270</td>\n      <td>40.27</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>332.5</td>\n      <td>142.5</td>\n      <td>0.0</td>\n      <td>228.0</td>\n      <td>0.0</td>\n      <td>932.0</td>\n      <td>594.0</td>\n      <td>365</td>\n      <td>41.05</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>198.6</td>\n      <td>132.4</td>\n      <td>0.0</td>\n      <td>192.0</td>\n      <td>0.0</td>\n      <td>978.4</td>\n      <td>825.5</td>\n      <td>360</td>\n      <td>44.30</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Check how many data points we have\nconcrete_data.shape",
            "execution_count": 876,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 876,
                    "data": {
                        "text/plain": "(1030, 9)"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Check data set for any missing values\nconcrete_data.describe()",
            "execution_count": 877,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 877,
                    "data": {
                        "text/plain": "            Cement  Blast Furnace Slag      Fly Ash        Water  \\\ncount  1030.000000         1030.000000  1030.000000  1030.000000   \nmean    281.167864           73.895825    54.188350   181.567282   \nstd     104.506364           86.279342    63.997004    21.354219   \nmin     102.000000            0.000000     0.000000   121.800000   \n25%     192.375000            0.000000     0.000000   164.900000   \n50%     272.900000           22.000000     0.000000   185.000000   \n75%     350.000000          142.950000   118.300000   192.000000   \nmax     540.000000          359.400000   200.100000   247.000000   \n\n       Superplasticizer  Coarse Aggregate  Fine Aggregate          Age  \\\ncount       1030.000000       1030.000000     1030.000000  1030.000000   \nmean           6.204660        972.918932      773.580485    45.662136   \nstd            5.973841         77.753954       80.175980    63.169912   \nmin            0.000000        801.000000      594.000000     1.000000   \n25%            0.000000        932.000000      730.950000     7.000000   \n50%            6.400000        968.000000      779.500000    28.000000   \n75%           10.200000       1029.400000      824.000000    56.000000   \nmax           32.200000       1145.000000      992.600000   365.000000   \n\n          Strength  \ncount  1030.000000  \nmean     35.817961  \nstd      16.705742  \nmin       2.330000  \n25%      23.710000  \n50%      34.445000  \n75%      46.135000  \nmax      82.600000  ",
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Cement</th>\n      <th>Blast Furnace Slag</th>\n      <th>Fly Ash</th>\n      <th>Water</th>\n      <th>Superplasticizer</th>\n      <th>Coarse Aggregate</th>\n      <th>Fine Aggregate</th>\n      <th>Age</th>\n      <th>Strength</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>1030.000000</td>\n      <td>1030.000000</td>\n      <td>1030.000000</td>\n      <td>1030.000000</td>\n      <td>1030.000000</td>\n      <td>1030.000000</td>\n      <td>1030.000000</td>\n      <td>1030.000000</td>\n      <td>1030.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>281.167864</td>\n      <td>73.895825</td>\n      <td>54.188350</td>\n      <td>181.567282</td>\n      <td>6.204660</td>\n      <td>972.918932</td>\n      <td>773.580485</td>\n      <td>45.662136</td>\n      <td>35.817961</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>104.506364</td>\n      <td>86.279342</td>\n      <td>63.997004</td>\n      <td>21.354219</td>\n      <td>5.973841</td>\n      <td>77.753954</td>\n      <td>80.175980</td>\n      <td>63.169912</td>\n      <td>16.705742</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>102.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>121.800000</td>\n      <td>0.000000</td>\n      <td>801.000000</td>\n      <td>594.000000</td>\n      <td>1.000000</td>\n      <td>2.330000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>192.375000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>164.900000</td>\n      <td>0.000000</td>\n      <td>932.000000</td>\n      <td>730.950000</td>\n      <td>7.000000</td>\n      <td>23.710000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>272.900000</td>\n      <td>22.000000</td>\n      <td>0.000000</td>\n      <td>185.000000</td>\n      <td>6.400000</td>\n      <td>968.000000</td>\n      <td>779.500000</td>\n      <td>28.000000</td>\n      <td>34.445000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>350.000000</td>\n      <td>142.950000</td>\n      <td>118.300000</td>\n      <td>192.000000</td>\n      <td>10.200000</td>\n      <td>1029.400000</td>\n      <td>824.000000</td>\n      <td>56.000000</td>\n      <td>46.135000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>540.000000</td>\n      <td>359.400000</td>\n      <td>200.100000</td>\n      <td>247.000000</td>\n      <td>32.200000</td>\n      <td>1145.000000</td>\n      <td>992.600000</td>\n      <td>365.000000</td>\n      <td>82.600000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Sum all NULL values per each column in the dataframe\nconcrete_data.isnull().sum()",
            "execution_count": 878,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 878,
                    "data": {
                        "text/plain": "Cement                0\nBlast Furnace Slag    0\nFly Ash               0\nWater                 0\nSuperplasticizer      0\nCoarse Aggregate      0\nFine Aggregate        0\nAge                   0\nStrength              0\ndtype: int64"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Split data into predictors and target='strenght'\nconcrete_data_columns=concrete_data.columns\n\n# Select All columns except 'Strenght' as our predictors\npredictors=concrete_data[concrete_data_columns[concrete_data_columns!='Strength']]\n\n# Get target columns as 'strenght'\ntarget=concrete_data['Strength']",
            "execution_count": 879,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Check predictors data frame\nprint(predictors.shape)\npredictors.head()",
            "execution_count": 880,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "(1030, 8)\n",
                    "name": "stdout"
                },
                {
                    "output_type": "execute_result",
                    "execution_count": 880,
                    "data": {
                        "text/plain": "   Cement  Blast Furnace Slag  Fly Ash  Water  Superplasticizer  \\\n0   540.0                 0.0      0.0  162.0               2.5   \n1   540.0                 0.0      0.0  162.0               2.5   \n2   332.5               142.5      0.0  228.0               0.0   \n3   332.5               142.5      0.0  228.0               0.0   \n4   198.6               132.4      0.0  192.0               0.0   \n\n   Coarse Aggregate  Fine Aggregate  Age  \n0            1040.0           676.0   28  \n1            1055.0           676.0   28  \n2             932.0           594.0  270  \n3             932.0           594.0  365  \n4             978.4           825.5  360  ",
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Cement</th>\n      <th>Blast Furnace Slag</th>\n      <th>Fly Ash</th>\n      <th>Water</th>\n      <th>Superplasticizer</th>\n      <th>Coarse Aggregate</th>\n      <th>Fine Aggregate</th>\n      <th>Age</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>540.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>162.0</td>\n      <td>2.5</td>\n      <td>1040.0</td>\n      <td>676.0</td>\n      <td>28</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>540.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>162.0</td>\n      <td>2.5</td>\n      <td>1055.0</td>\n      <td>676.0</td>\n      <td>28</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>332.5</td>\n      <td>142.5</td>\n      <td>0.0</td>\n      <td>228.0</td>\n      <td>0.0</td>\n      <td>932.0</td>\n      <td>594.0</td>\n      <td>270</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>332.5</td>\n      <td>142.5</td>\n      <td>0.0</td>\n      <td>228.0</td>\n      <td>0.0</td>\n      <td>932.0</td>\n      <td>594.0</td>\n      <td>365</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>198.6</td>\n      <td>132.4</td>\n      <td>0.0</td>\n      <td>192.0</td>\n      <td>0.0</td>\n      <td>978.4</td>\n      <td>825.5</td>\n      <td>360</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Check target data frame\nprint(target.shape)\ntarget.head()",
            "execution_count": 881,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "(1030,)\n",
                    "name": "stdout"
                },
                {
                    "output_type": "execute_result",
                    "execution_count": 881,
                    "data": {
                        "text/plain": "0    79.99\n1    61.89\n2    40.27\n3    41.05\n4    44.30\nName: Strength, dtype: float64"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Randomly split the data into a training and test sets by holding 30% of the data for testing\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test=train_test_split(predictors, target, test_size=0.3, random_state=4)\n\n# Show new created training and test set shape\nprint('Full Data set:', predictors.shape, target.shape)\nprint('Training set:', X_train.shape, y_train.shape)\nprint('Test set:', X_test.shape, y_test.shape)",
            "execution_count": 882,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "Full Data set: (1030, 8) (1030,)\nTraining set: (721, 8) (721,)\nTest set: (309, 8) (309,)\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Save the number of predictors to 'n_cols'. We'll need this number when building the network.\nn_cols=predictors.shape[1]\nn_cols",
            "execution_count": 883,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 883,
                    "data": {
                        "text/plain": "8"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Data looks clean and ready to build our model.\n# Let\u00b4s import the Keras Library \nimport keras\n\n# Import the keras packages to build our regression model\nfrom keras.models import Sequential # 'Sequential' model constructor\nfrom keras.layers import Dense      # We use 'Dense' type layers in our network",
            "execution_count": 884,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Define regression model function\ndef regression_model():\n    # Create model\n    model=Sequential()\n    \n    # Add One hidden layer with 10 nodes/neurons each one and ReLU activation function\n    model.add(Dense(10, activation='relu', input_shape=(n_cols,)))\n    \n    # Add Output layer with 1 node/neuron\n    model.add(Dense(1))\n    \n    # Compile model adding adam Optimizer and the mean squared error as the loss function.\n    model.compile(optimizer='adam', loss='mean_squared_error')\n    return model",
            "execution_count": 885,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Build our model\nmodel=regression_model()\n\n# Train/fit the model on the training set using 50 epochs, and validate the model on test set.\nerror=model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, verbose=2)",
            "execution_count": 886,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "Train on 721 samples, validate on 309 samples\nEpoch 1/50\n - 9s - loss: 106006.3928 - val_loss: 63513.9886\nEpoch 2/50\n - 0s - loss: 39472.5595 - val_loss: 20451.2612\nEpoch 3/50\n - 0s - loss: 11557.9644 - val_loss: 5488.9410\nEpoch 4/50\n - 0s - loss: 3003.6886 - val_loss: 1895.4335\nEpoch 5/50\n - 0s - loss: 1553.9954 - val_loss: 1588.5794\nEpoch 6/50\n - 0s - loss: 1428.5663 - val_loss: 1510.1569\nEpoch 7/50\n - 0s - loss: 1350.9323 - val_loss: 1437.3049\nEpoch 8/50\n - 0s - loss: 1282.0127 - val_loss: 1362.2999\nEpoch 9/50\n - 0s - loss: 1210.5058 - val_loss: 1286.1179\nEpoch 10/50\n - 0s - loss: 1134.0123 - val_loss: 1209.5958\nEpoch 11/50\n - 0s - loss: 1068.6524 - val_loss: 1135.5245\nEpoch 12/50\n - 0s - loss: 1000.5384 - val_loss: 1066.7246\nEpoch 13/50\n - 0s - loss: 933.2642 - val_loss: 997.2015\nEpoch 14/50\n - 0s - loss: 870.1212 - val_loss: 933.1631\nEpoch 15/50\n - 0s - loss: 808.0316 - val_loss: 872.4827\nEpoch 16/50\n - 0s - loss: 753.6012 - val_loss: 810.2669\nEpoch 17/50\n - 0s - loss: 697.6107 - val_loss: 755.1780\nEpoch 18/50\n - 0s - loss: 644.8034 - val_loss: 700.1733\nEpoch 19/50\n - 0s - loss: 596.9273 - val_loss: 650.3184\nEpoch 20/50\n - 0s - loss: 550.2827 - val_loss: 607.6940\nEpoch 21/50\n - 0s - loss: 509.6762 - val_loss: 559.7735\nEpoch 22/50\n - 0s - loss: 469.1961 - val_loss: 517.5741\nEpoch 23/50\n - 0s - loss: 432.8597 - val_loss: 485.8559\nEpoch 24/50\n - 0s - loss: 402.3258 - val_loss: 445.6961\nEpoch 25/50\n - 0s - loss: 372.2652 - val_loss: 417.2095\nEpoch 26/50\n - 0s - loss: 344.5198 - val_loss: 389.5976\nEpoch 27/50\n - 0s - loss: 322.4973 - val_loss: 359.1496\nEpoch 28/50\n - 0s - loss: 298.1496 - val_loss: 340.2803\nEpoch 29/50\n - 0s - loss: 278.7026 - val_loss: 314.2179\nEpoch 30/50\n - 0s - loss: 259.4392 - val_loss: 295.0782\nEpoch 31/50\n - 0s - loss: 242.3927 - val_loss: 277.7492\nEpoch 32/50\n - 0s - loss: 226.5810 - val_loss: 261.3288\nEpoch 33/50\n - 0s - loss: 212.8410 - val_loss: 246.3692\nEpoch 34/50\n - 0s - loss: 199.8111 - val_loss: 232.5497\nEpoch 35/50\n - 0s - loss: 189.3905 - val_loss: 220.3561\nEpoch 36/50\n - 0s - loss: 178.4199 - val_loss: 210.6602\nEpoch 37/50\n - 0s - loss: 170.6522 - val_loss: 208.0912\nEpoch 38/50\n - 0s - loss: 161.3302 - val_loss: 190.5554\nEpoch 39/50\n - 0s - loss: 154.5849 - val_loss: 187.0387\nEpoch 40/50\n - 0s - loss: 147.9493 - val_loss: 180.6841\nEpoch 41/50\n - 0s - loss: 140.8841 - val_loss: 170.0894\nEpoch 42/50\n - 0s - loss: 135.6935 - val_loss: 164.1854\nEpoch 43/50\n - 0s - loss: 130.3970 - val_loss: 162.8757\nEpoch 44/50\n - 0s - loss: 126.8312 - val_loss: 153.5678\nEpoch 45/50\n - 0s - loss: 122.5404 - val_loss: 149.4322\nEpoch 46/50\n - 0s - loss: 118.8554 - val_loss: 148.5708\nEpoch 47/50\n - 0s - loss: 115.5963 - val_loss: 141.8730\nEpoch 48/50\n - 0s - loss: 113.0705 - val_loss: 140.7208\nEpoch 49/50\n - 0s - loss: 110.4362 - val_loss: 137.3065\nEpoch 50/50\n - 0s - loss: 108.1974 - val_loss: 137.6999\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Use .predict() method to get an array of new predicted values of 'strenght' using test set\ny_hat=model.predict(X_test)\n\n# Create data frame for comparing true Strength and Predicted Strength\ndf=pd.DataFrame(y_hat, y_test)\ndf.reset_index(inplace=True)\ndf.columns=['Strength', 'Predicted Strength']\n\n#Select first row as sample to report\ny_testA=df.iloc[0,0]\ny_hatA=df.iloc[0,1]\n\n\ndf.head()",
            "execution_count": 887,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 887,
                    "data": {
                        "text/plain": "   Strength  Predicted Strength\n0     44.52           40.439449\n1     50.53           46.148174\n2     21.82           42.090893\n3     38.80           40.280331\n4     55.60           61.221523",
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Strength</th>\n      <th>Predicted Strength</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>44.52</td>\n      <td>40.439449</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>50.53</td>\n      <td>46.148174</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>21.82</td>\n      <td>42.090893</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>38.80</td>\n      <td>40.280331</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>55.60</td>\n      <td>61.221523</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Evaluate the model\nscores=model.evaluate(X_test, y_test, verbose=0)\n\n# Use Scikit-learn to corroborate that our MSE value between true Strength and Predicted Strength is correct\nfrom sklearn.metrics import mean_squared_error\n\nMSE=mean_squared_error(y_test, y_hat)\nMSEA=MSE     # To report\n\nprint('We get the last Mean Squared Error value on test set at Epoch 50/50 such as: {},'.format(np.around(scores, decimals=4)))\nprint('and the same value: {} computed with Scikit-learn.'.format(np.around(MSE, decimals=4)))",
            "execution_count": 888,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "We get the last Mean Squared Error value on test set at Epoch 50/50 such as: 137.6999,\nand the same value: 137.6999 computed with Scikit-learn.\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {
                "scrolled": true
            },
            "cell_type": "code",
            "source": "# Extract/Get out the MSE values of our History object as dictionary using .history attribute\ndictionary=error.history\n\n# Select each array of values in the dictionary\nloss_train=dictionary['loss']\nval_loss_test=dictionary['val_loss']\n\n# Create a Data Frame displaying the 50 MSE values at train and test set\nerror_values=pd.DataFrame(val_loss_test, loss_train)\nerror_values.reset_index(inplace=True)\nerror_values.columns=['loss (train set)','validated loss (test set)']\nerror_values.head(50)",
            "execution_count": 889,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 889,
                    "data": {
                        "text/plain": "    loss (train set)  validated loss (test set)\n0      106006.392770               63513.988572\n1       39472.559485               20451.261163\n2       11557.964432                5488.941025\n3        3003.688636                1895.433484\n4        1553.995355                1588.579362\n5        1428.566340                1510.156883\n6        1350.932338                1437.304950\n7        1282.012746                1362.299910\n8        1210.505848                1286.117879\n9        1134.012302                1209.595817\n10       1068.652434                1135.524522\n11       1000.538446                1066.724562\n12        933.264226                 997.201533\n13        870.121162                 933.163106\n14        808.031633                 872.482735\n15        753.601173                 810.266872\n16        697.610656                 755.178026\n17        644.803448                 700.173269\n18        596.927261                 650.318427\n19        550.282654                 607.693971\n20        509.676207                 559.773474\n21        469.196124                 517.574059\n22        432.859694                 485.855926\n23        402.325779                 445.696109\n24        372.265150                 417.209527\n25        344.519766                 389.597594\n26        322.497253                 359.149609\n27        298.149579                 340.280278\n28        278.702587                 314.217938\n29        259.439248                 295.078175\n30        242.392699                 277.749210\n31        226.581049                 261.328814\n32        212.841030                 246.369250\n33        199.811104                 232.549666\n34        189.390464                 220.356057\n35        178.419930                 210.660198\n36        170.652235                 208.091237\n37        161.330153                 190.555429\n38        154.584942                 187.038653\n39        147.949307                 180.684079\n40        140.884113                 170.089424\n41        135.693468                 164.185388\n42        130.397034                 162.875728\n43        126.831212                 153.567789\n44        122.540378                 149.432172\n45        118.855432                 148.570848\n46        115.596336                 141.872970\n47        113.070544                 140.720845\n48        110.436180                 137.306547\n49        108.197374                 137.699864",
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>loss (train set)</th>\n      <th>validated loss (test set)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>106006.392770</td>\n      <td>63513.988572</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>39472.559485</td>\n      <td>20451.261163</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>11557.964432</td>\n      <td>5488.941025</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3003.688636</td>\n      <td>1895.433484</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1553.995355</td>\n      <td>1588.579362</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1428.566340</td>\n      <td>1510.156883</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>1350.932338</td>\n      <td>1437.304950</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>1282.012746</td>\n      <td>1362.299910</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>1210.505848</td>\n      <td>1286.117879</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>1134.012302</td>\n      <td>1209.595817</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>1068.652434</td>\n      <td>1135.524522</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>1000.538446</td>\n      <td>1066.724562</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>933.264226</td>\n      <td>997.201533</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>870.121162</td>\n      <td>933.163106</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>808.031633</td>\n      <td>872.482735</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>753.601173</td>\n      <td>810.266872</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>697.610656</td>\n      <td>755.178026</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>644.803448</td>\n      <td>700.173269</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>596.927261</td>\n      <td>650.318427</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>550.282654</td>\n      <td>607.693971</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>509.676207</td>\n      <td>559.773474</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>469.196124</td>\n      <td>517.574059</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>432.859694</td>\n      <td>485.855926</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>402.325779</td>\n      <td>445.696109</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>372.265150</td>\n      <td>417.209527</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>344.519766</td>\n      <td>389.597594</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>322.497253</td>\n      <td>359.149609</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>298.149579</td>\n      <td>340.280278</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>278.702587</td>\n      <td>314.217938</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>259.439248</td>\n      <td>295.078175</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>242.392699</td>\n      <td>277.749210</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>226.581049</td>\n      <td>261.328814</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>212.841030</td>\n      <td>246.369250</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>199.811104</td>\n      <td>232.549666</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>189.390464</td>\n      <td>220.356057</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>178.419930</td>\n      <td>210.660198</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>170.652235</td>\n      <td>208.091237</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>161.330153</td>\n      <td>190.555429</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>154.584942</td>\n      <td>187.038653</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>147.949307</td>\n      <td>180.684079</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>140.884113</td>\n      <td>170.089424</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>135.693468</td>\n      <td>164.185388</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>130.397034</td>\n      <td>162.875728</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>126.831212</td>\n      <td>153.567789</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>122.540378</td>\n      <td>149.432172</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>118.855432</td>\n      <td>148.570848</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>115.596336</td>\n      <td>141.872970</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>113.070544</td>\n      <td>140.720845</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>110.436180</td>\n      <td>137.306547</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>108.197374</td>\n      <td>137.699864</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Report the mean and the standard deviation of the mean squared errors\n# Get Statistics with .describe() method and select just mean and std in 'error_values' data frame\nstats=error_values.describe()\nmeanA=stats.iloc[1,1] # To report\nstats[1:3]",
            "execution_count": 890,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 890,
                    "data": {
                        "text/plain": "      loss (train set)  validated loss (test set)\nmean       3654.410994                2322.370858\nstd       15836.846043                9300.250886",
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>loss (train set)</th>\n      <th>validated loss (test set)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>mean</th>\n      <td>3654.410994</td>\n      <td>2322.370858</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>15836.846043</td>\n      <td>9300.250886</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "<a id='itemB'></a>\n### B. Repeat Part A but use a normalized version of the data."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Normalize the inputs/predictors in training set.\nX_train_norm=(X_train - X_train.mean()) / X_train.std()\nX_train_norm.head()",
            "execution_count": 891,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 891,
                    "data": {
                        "text/plain": "       Cement  Blast Furnace Slag   Fly Ash     Water  Superplasticizer  \\\n401  1.792477           -0.842033 -0.853275 -0.934934          0.545015   \n288 -0.932850           -0.842033  1.737310 -0.578668          0.230955   \n406 -1.085082           -0.842033  1.374318 -0.850555         -1.025288   \n945 -1.318072            0.712067  0.749165  0.860457          0.197896   \n616 -0.045448           -0.842033 -0.853275  0.424501         -1.025288   \n\n     Coarse Aggregate  Fine Aggregate       Age  \n401          0.885941       -1.326981 -0.302526  \n288          1.033146        0.066649  0.847751  \n406          0.398645        1.548133 -0.701928  \n945         -0.738382       -0.227000 -0.302526  \n616         -0.078500        1.007770  5.001532  ",
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Cement</th>\n      <th>Blast Furnace Slag</th>\n      <th>Fly Ash</th>\n      <th>Water</th>\n      <th>Superplasticizer</th>\n      <th>Coarse Aggregate</th>\n      <th>Fine Aggregate</th>\n      <th>Age</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>401</th>\n      <td>1.792477</td>\n      <td>-0.842033</td>\n      <td>-0.853275</td>\n      <td>-0.934934</td>\n      <td>0.545015</td>\n      <td>0.885941</td>\n      <td>-1.326981</td>\n      <td>-0.302526</td>\n    </tr>\n    <tr>\n      <th>288</th>\n      <td>-0.932850</td>\n      <td>-0.842033</td>\n      <td>1.737310</td>\n      <td>-0.578668</td>\n      <td>0.230955</td>\n      <td>1.033146</td>\n      <td>0.066649</td>\n      <td>0.847751</td>\n    </tr>\n    <tr>\n      <th>406</th>\n      <td>-1.085082</td>\n      <td>-0.842033</td>\n      <td>1.374318</td>\n      <td>-0.850555</td>\n      <td>-1.025288</td>\n      <td>0.398645</td>\n      <td>1.548133</td>\n      <td>-0.701928</td>\n    </tr>\n    <tr>\n      <th>945</th>\n      <td>-1.318072</td>\n      <td>0.712067</td>\n      <td>0.749165</td>\n      <td>0.860457</td>\n      <td>0.197896</td>\n      <td>-0.738382</td>\n      <td>-0.227000</td>\n      <td>-0.302526</td>\n    </tr>\n    <tr>\n      <th>616</th>\n      <td>-0.045448</td>\n      <td>-0.842033</td>\n      <td>-0.853275</td>\n      <td>0.424501</td>\n      <td>-1.025288</td>\n      <td>-0.078500</td>\n      <td>1.007770</td>\n      <td>5.001532</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Normalize the inputs/predictors in test set.\nX_test_norm=(X_test - X_test.mean()) / X_test.std()\nX_test_norm.head()",
            "execution_count": 892,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 892,
                    "data": {
                        "text/plain": "       Cement  Blast Furnace Slag   Fly Ash     Water  Superplasticizer  \\\n522  0.046854           -0.720173  1.409228 -0.078799         -0.122201   \n701  0.088213            1.281494 -0.830446  0.528287         -1.070083   \n563 -0.711057            2.684923 -0.830446  0.234084         -1.070083   \n678  0.088213            1.281494 -0.830446  0.528287         -1.070083   \n98   2.021765            0.453686 -0.830446  0.019269          0.463762   \n\n     Coarse Aggregate  Fine Aggregate       Age  \n522         -1.699502        0.333673  0.206369  \n701         -0.504189       -0.806759  0.733507  \n563          0.093467       -1.197411 -0.553329  \n678         -0.504189       -0.806759 -0.227744  \n98          -1.565362        0.066384 -0.553329  ",
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Cement</th>\n      <th>Blast Furnace Slag</th>\n      <th>Fly Ash</th>\n      <th>Water</th>\n      <th>Superplasticizer</th>\n      <th>Coarse Aggregate</th>\n      <th>Fine Aggregate</th>\n      <th>Age</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>522</th>\n      <td>0.046854</td>\n      <td>-0.720173</td>\n      <td>1.409228</td>\n      <td>-0.078799</td>\n      <td>-0.122201</td>\n      <td>-1.699502</td>\n      <td>0.333673</td>\n      <td>0.206369</td>\n    </tr>\n    <tr>\n      <th>701</th>\n      <td>0.088213</td>\n      <td>1.281494</td>\n      <td>-0.830446</td>\n      <td>0.528287</td>\n      <td>-1.070083</td>\n      <td>-0.504189</td>\n      <td>-0.806759</td>\n      <td>0.733507</td>\n    </tr>\n    <tr>\n      <th>563</th>\n      <td>-0.711057</td>\n      <td>2.684923</td>\n      <td>-0.830446</td>\n      <td>0.234084</td>\n      <td>-1.070083</td>\n      <td>0.093467</td>\n      <td>-1.197411</td>\n      <td>-0.553329</td>\n    </tr>\n    <tr>\n      <th>678</th>\n      <td>0.088213</td>\n      <td>1.281494</td>\n      <td>-0.830446</td>\n      <td>0.528287</td>\n      <td>-1.070083</td>\n      <td>-0.504189</td>\n      <td>-0.806759</td>\n      <td>-0.227744</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>2.021765</td>\n      <td>0.453686</td>\n      <td>-0.830446</td>\n      <td>0.019269</td>\n      <td>0.463762</td>\n      <td>-1.565362</td>\n      <td>0.066384</td>\n      <td>-0.553329</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Build our model\nmodel=regression_model()\n\n# Train/fit the model on the training set using 50 epochs, and validate the model on test set.\nerror=model.fit(X_train_norm, y_train, validation_data=(X_test_norm, y_test), epochs=50, verbose=2)",
            "execution_count": 893,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "Train on 721 samples, validate on 309 samples\nEpoch 1/50\n - 8s - loss: 1568.5704 - val_loss: 1504.4738\nEpoch 2/50\n - 0s - loss: 1554.5798 - val_loss: 1491.1339\nEpoch 3/50\n - 0s - loss: 1540.5810 - val_loss: 1477.8037\nEpoch 4/50\n - 0s - loss: 1526.5726 - val_loss: 1464.1372\nEpoch 5/50\n - 0s - loss: 1511.9989 - val_loss: 1450.2452\nEpoch 6/50\n - 0s - loss: 1497.0109 - val_loss: 1435.6246\nEpoch 7/50\n - 0s - loss: 1481.2603 - val_loss: 1420.5208\nEpoch 8/50\n - 0s - loss: 1464.8205 - val_loss: 1404.2346\nEpoch 9/50\n - 0s - loss: 1447.2504 - val_loss: 1387.1105\nEpoch 10/50\n - 0s - loss: 1428.6036 - val_loss: 1368.8448\nEpoch 11/50\n - 0s - loss: 1408.8812 - val_loss: 1349.4705\nEpoch 12/50\n - 0s - loss: 1388.0260 - val_loss: 1329.0920\nEpoch 13/50\n - 0s - loss: 1366.0374 - val_loss: 1307.5961\nEpoch 14/50\n - 0s - loss: 1342.9561 - val_loss: 1285.2853\nEpoch 15/50\n - 1s - loss: 1318.8044 - val_loss: 1261.9018\nEpoch 16/50\n - 1s - loss: 1293.3602 - val_loss: 1237.7852\nEpoch 17/50\n - 0s - loss: 1267.1831 - val_loss: 1212.4126\nEpoch 18/50\n - 0s - loss: 1239.7676 - val_loss: 1186.2646\nEpoch 19/50\n - 0s - loss: 1211.5862 - val_loss: 1158.9907\nEpoch 20/50\n - 0s - loss: 1182.1703 - val_loss: 1130.9954\nEpoch 21/50\n - 0s - loss: 1151.5408 - val_loss: 1102.7557\nEpoch 22/50\n - 0s - loss: 1120.5142 - val_loss: 1072.6924\nEpoch 23/50\n - 0s - loss: 1087.8774 - val_loss: 1042.1339\nEpoch 24/50\n - 0s - loss: 1054.5138 - val_loss: 1010.5291\nEpoch 25/50\n - 0s - loss: 1020.1498 - val_loss: 978.2412\nEpoch 26/50\n - 0s - loss: 984.6063 - val_loss: 945.3682\nEpoch 27/50\n - 0s - loss: 948.4863 - val_loss: 911.5763\nEpoch 28/50\n - 0s - loss: 911.8564 - val_loss: 876.9492\nEpoch 29/50\n - 0s - loss: 874.4118 - val_loss: 842.2416\nEpoch 30/50\n - 0s - loss: 836.8974 - val_loss: 807.6956\nEpoch 31/50\n - 0s - loss: 799.3215 - val_loss: 773.0433\nEpoch 32/50\n - 0s - loss: 762.1284 - val_loss: 738.8192\nEpoch 33/50\n - 1s - loss: 725.4317 - val_loss: 705.0874\nEpoch 34/50\n - 0s - loss: 689.4010 - val_loss: 672.0005\nEpoch 35/50\n - 0s - loss: 654.1635 - val_loss: 640.2136\nEpoch 36/50\n - 0s - loss: 620.4780 - val_loss: 608.5582\nEpoch 37/50\n - 0s - loss: 587.4876 - val_loss: 578.4961\nEpoch 38/50\n - 0s - loss: 555.8927 - val_loss: 550.0834\nEpoch 39/50\n - 0s - loss: 525.7575 - val_loss: 522.2738\nEpoch 40/50\n - 0s - loss: 496.8763 - val_loss: 496.8806\nEpoch 41/50\n - 0s - loss: 470.1508 - val_loss: 472.1402\nEpoch 42/50\n - 0s - loss: 444.2776 - val_loss: 449.2099\nEpoch 43/50\n - 0s - loss: 420.2668 - val_loss: 427.0723\nEpoch 44/50\n - 0s - loss: 397.2282 - val_loss: 406.9881\nEpoch 45/50\n - 0s - loss: 376.2478 - val_loss: 387.5850\nEpoch 46/50\n - 0s - loss: 356.5675 - val_loss: 370.0133\nEpoch 47/50\n - 0s - loss: 338.4945 - val_loss: 353.6003\nEpoch 48/50\n - 0s - loss: 321.7345 - val_loss: 338.2704\nEpoch 49/50\n - 0s - loss: 306.1692 - val_loss: 324.8391\nEpoch 50/50\n - 0s - loss: 292.2779 - val_loss: 311.9161\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Use .predict() method to get an array of new predicted values of 'strenght' using test set\ny_hat=model.predict(X_test_norm)\n\n# Create data frame for comparing true Strength and Predicted Strength\ndf=pd.DataFrame(y_hat, y_test)\ndf.reset_index(inplace=True)\ndf.columns=['Strength', 'Predicted Strength']\n\n#Select first row as sample to report\ny_testB=df.iloc[0,0]\ny_hatB=df.iloc[0,1]\n\ndf.head()",
            "execution_count": 894,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 894,
                    "data": {
                        "text/plain": "   Strength  Predicted Strength\n0     44.52           27.473602\n1     50.53           29.726318\n2     21.82           32.074711\n3     38.80           27.715340\n4     55.60           36.944599",
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Strength</th>\n      <th>Predicted Strength</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>44.52</td>\n      <td>27.473602</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>50.53</td>\n      <td>29.726318</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>21.82</td>\n      <td>32.074711</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>38.80</td>\n      <td>27.715340</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>55.60</td>\n      <td>36.944599</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Evaluate the model\nscores=model.evaluate(X_test_norm, y_test, verbose=0)\n\n# Use Scikit-learn to corroborate that our MSE value between true Strength and Predicted Strength is correct\nfrom sklearn.metrics import mean_squared_error\n\nMSE=mean_squared_error(y_test, y_hat)\nMSEB=MSE   # To report\n\nprint('We get the last Mean Squared Error value on test set at Epoch 50/50 such as: {},'.format(np.around(scores, decimals=4)))\nprint('and the same value: {} computed with Scikit-learn, and it has INCREASED compared to step A.'.format(np.around(MSE, decimals=4)))",
            "execution_count": 895,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "We get the last Mean Squared Error value on test set at Epoch 50/50 such as: 311.9161,\nand the same value: 311.9161 computed with Scikit-learn, and it has INCREASED compared to step A.\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Extract/Get out the MSE values of our History object as dictionary using .history attribute\ndictionary=error.history\n\n# Select each array of values in the dictionary\nloss_train=dictionary['loss']\nval_loss_test=dictionary['val_loss']\n\n# Create a Data Frame displaying the 50 MSE values at train and test set\nerror_values=pd.DataFrame(val_loss_test, loss_train)\nerror_values.reset_index(inplace=True)\nerror_values.columns=['loss (train set)','validated loss (test set)']\nerror_values.head(50)",
            "execution_count": 896,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 896,
                    "data": {
                        "text/plain": "    loss (train set)  validated loss (test set)\n0        1568.570357                1504.473831\n1        1554.579803                1491.133865\n2        1540.581046                1477.803674\n3        1526.572587                1464.137177\n4        1511.998912                1450.245159\n5        1497.010886                1435.624647\n6        1481.260323                1420.520828\n7        1464.820463                1404.234571\n8        1447.250401                1387.110547\n9        1428.603563                1368.844789\n10       1408.881214                1349.470467\n11       1388.025976                1329.092002\n12       1366.037353                1307.596087\n13       1342.956123                1285.285341\n14       1318.804414                1261.901845\n15       1293.360243                1237.785195\n16       1267.183113                1212.412641\n17       1239.767561                1186.264640\n18       1211.586249                1158.990749\n19       1182.170309                1130.995400\n20       1151.540782                1102.755674\n21       1120.514180                1072.692394\n22       1087.877416                1042.133858\n23       1054.513755                1010.529136\n24       1020.149753                 978.241224\n25        984.606272                 945.368190\n26        948.486344                 911.576275\n27        911.856387                 876.949188\n28        874.411771                 842.241568\n29        836.897394                 807.695634\n30        799.321460                 773.043343\n31        762.128375                 738.819155\n32        725.431703                 705.087374\n33        689.401015                 672.000497\n34        654.163537                 640.213649\n35        620.477975                 608.558241\n36        587.487581                 578.496136\n37        555.892748                 550.083405\n38        525.757467                 522.273795\n39        496.876324                 496.880559\n40        470.150790                 472.140172\n41        444.277575                 449.209946\n42        420.266820                 427.072308\n43        397.228214                 406.988095\n44        376.247752                 387.584977\n45        356.567452                 370.013338\n46        338.494510                 353.600273\n47        321.734502                 338.270401\n48        306.169159                 324.839055\n49        292.277922                 311.916112",
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>loss (train set)</th>\n      <th>validated loss (test set)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1568.570357</td>\n      <td>1504.473831</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1554.579803</td>\n      <td>1491.133865</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1540.581046</td>\n      <td>1477.803674</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1526.572587</td>\n      <td>1464.137177</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1511.998912</td>\n      <td>1450.245159</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1497.010886</td>\n      <td>1435.624647</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>1481.260323</td>\n      <td>1420.520828</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>1464.820463</td>\n      <td>1404.234571</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>1447.250401</td>\n      <td>1387.110547</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>1428.603563</td>\n      <td>1368.844789</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>1408.881214</td>\n      <td>1349.470467</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>1388.025976</td>\n      <td>1329.092002</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>1366.037353</td>\n      <td>1307.596087</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>1342.956123</td>\n      <td>1285.285341</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>1318.804414</td>\n      <td>1261.901845</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>1293.360243</td>\n      <td>1237.785195</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>1267.183113</td>\n      <td>1212.412641</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>1239.767561</td>\n      <td>1186.264640</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>1211.586249</td>\n      <td>1158.990749</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>1182.170309</td>\n      <td>1130.995400</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>1151.540782</td>\n      <td>1102.755674</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>1120.514180</td>\n      <td>1072.692394</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>1087.877416</td>\n      <td>1042.133858</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>1054.513755</td>\n      <td>1010.529136</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>1020.149753</td>\n      <td>978.241224</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>984.606272</td>\n      <td>945.368190</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>948.486344</td>\n      <td>911.576275</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>911.856387</td>\n      <td>876.949188</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>874.411771</td>\n      <td>842.241568</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>836.897394</td>\n      <td>807.695634</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>799.321460</td>\n      <td>773.043343</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>762.128375</td>\n      <td>738.819155</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>725.431703</td>\n      <td>705.087374</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>689.401015</td>\n      <td>672.000497</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>654.163537</td>\n      <td>640.213649</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>620.477975</td>\n      <td>608.558241</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>587.487581</td>\n      <td>578.496136</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>555.892748</td>\n      <td>550.083405</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>525.757467</td>\n      <td>522.273795</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>496.876324</td>\n      <td>496.880559</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>470.150790</td>\n      <td>472.140172</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>444.277575</td>\n      <td>449.209946</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>420.266820</td>\n      <td>427.072308</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>397.228214</td>\n      <td>406.988095</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>376.247752</td>\n      <td>387.584977</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>356.567452</td>\n      <td>370.013338</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>338.494510</td>\n      <td>353.600273</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>321.734502</td>\n      <td>338.270401</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>306.169159</td>\n      <td>324.839055</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>292.277922</td>\n      <td>311.916112</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Report the mean and the standard deviation of the mean squared errors\n# Get Statistics with .describe() method and select just mean and std in 'error_values' data frame\nstats=error_values.describe()\nmeanB=stats.iloc[1,1] # To report\nstats[1:3]",
            "execution_count": 897,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 897,
                    "data": {
                        "text/plain": "      loss (train set)  validated loss (test set)\nmean        963.424557                 931.623949\nstd         425.926999                 397.989829",
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>loss (train set)</th>\n      <th>validated loss (test set)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>mean</th>\n      <td>963.424557</td>\n      <td>931.623949</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>425.926999</td>\n      <td>397.989829</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "#### The *mean* of Mean Squared Error has DECREASED on step B, compared to step A."
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "<a id='itemC'></a>\n### C. Increate the number of epochs\n#### Repeat Part B but use 100 epochs this time for training."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Build our model\nmodel=regression_model()\n\n# Train/fit the model on the training set using 50 epochs, and validate the model on test set.\nerror=model.fit(X_train_norm, y_train, validation_data=(X_test_norm, y_test), epochs=100, verbose=2)",
            "execution_count": 898,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "Train on 721 samples, validate on 309 samples\nEpoch 1/100\n - 8s - loss: 1618.1457 - val_loss: 1555.0297\nEpoch 2/100\n - 0s - loss: 1605.0979 - val_loss: 1542.7327\nEpoch 3/100\n - 0s - loss: 1592.2167 - val_loss: 1530.6456\nEpoch 4/100\n - 0s - loss: 1579.5535 - val_loss: 1518.6370\nEpoch 5/100\n - 0s - loss: 1566.7230 - val_loss: 1506.3661\nEpoch 6/100\n - 0s - loss: 1553.4323 - val_loss: 1493.6529\nEpoch 7/100\n - 0s - loss: 1539.5092 - val_loss: 1480.3573\nEpoch 8/100\n - 0s - loss: 1524.8621 - val_loss: 1466.1366\nEpoch 9/100\n - 0s - loss: 1509.1276 - val_loss: 1451.0277\nEpoch 10/100\n - 0s - loss: 1492.4963 - val_loss: 1434.7579\nEpoch 11/100\n - 0s - loss: 1474.4569 - val_loss: 1417.3850\nEpoch 12/100\n - 0s - loss: 1454.9961 - val_loss: 1398.4876\nEpoch 13/100\n - 0s - loss: 1433.8540 - val_loss: 1377.8350\nEpoch 14/100\n - 0s - loss: 1411.0470 - val_loss: 1355.6406\nEpoch 15/100\n - 0s - loss: 1386.4724 - val_loss: 1331.8139\nEpoch 16/100\n - 0s - loss: 1360.2431 - val_loss: 1305.9919\nEpoch 17/100\n - 0s - loss: 1332.0487 - val_loss: 1278.9287\nEpoch 18/100\n - 0s - loss: 1302.6808 - val_loss: 1250.5997\nEpoch 19/100\n - 1s - loss: 1271.7219 - val_loss: 1221.3169\nEpoch 20/100\n - 1s - loss: 1239.7850 - val_loss: 1190.3765\nEpoch 21/100\n - 0s - loss: 1206.7657 - val_loss: 1158.9110\nEpoch 22/100\n - 0s - loss: 1173.2370 - val_loss: 1126.8162\nEpoch 23/100\n - 0s - loss: 1138.5690 - val_loss: 1095.1447\nEpoch 24/100\n - 0s - loss: 1104.2707 - val_loss: 1061.3968\nEpoch 25/100\n - 0s - loss: 1068.4440 - val_loss: 1027.6475\nEpoch 26/100\n - 0s - loss: 1032.5587 - val_loss: 993.8390\nEpoch 27/100\n - 0s - loss: 996.9102 - val_loss: 958.5944\nEpoch 28/100\n - 0s - loss: 959.8704 - val_loss: 924.7026\nEpoch 29/100\n - 0s - loss: 923.6659 - val_loss: 889.5586\nEpoch 30/100\n - 0s - loss: 886.5245 - val_loss: 855.9529\nEpoch 31/100\n - 0s - loss: 850.3200 - val_loss: 821.1323\nEpoch 32/100\n - 0s - loss: 814.0125 - val_loss: 787.1473\nEpoch 33/100\n - 0s - loss: 778.2132 - val_loss: 754.1225\nEpoch 34/100\n - 0s - loss: 743.2134 - val_loss: 721.4674\nEpoch 35/100\n - 0s - loss: 708.6857 - val_loss: 689.6934\nEpoch 36/100\n - 0s - loss: 675.3169 - val_loss: 658.4279\nEpoch 37/100\n - 0s - loss: 642.1004 - val_loss: 628.8539\nEpoch 38/100\n - 0s - loss: 610.8485 - val_loss: 600.3387\nEpoch 39/100\n - 0s - loss: 581.1114 - val_loss: 572.2094\nEpoch 40/100\n - 0s - loss: 551.8664 - val_loss: 546.2718\nEpoch 41/100\n - 0s - loss: 524.5677 - val_loss: 521.3157\nEpoch 42/100\n - 0s - loss: 498.4713 - val_loss: 498.0442\nEpoch 43/100\n - 0s - loss: 474.1488 - val_loss: 475.7437\nEpoch 44/100\n - 0s - loss: 450.7556 - val_loss: 455.1992\nEpoch 45/100\n - 0s - loss: 429.2893 - val_loss: 435.2974\nEpoch 46/100\n - 0s - loss: 408.7356 - val_loss: 416.8725\nEpoch 47/100\n - 0s - loss: 389.3421 - val_loss: 400.4240\nEpoch 48/100\n - 0s - loss: 371.9039 - val_loss: 383.9776\nEpoch 49/100\n - 0s - loss: 355.0368 - val_loss: 369.4796\nEpoch 50/100\n - 0s - loss: 339.7802 - val_loss: 355.7604\nEpoch 51/100\n - 0s - loss: 325.5399 - val_loss: 342.9569\nEpoch 52/100\n - 0s - loss: 312.2485 - val_loss: 331.1136\nEpoch 53/100\n - 0s - loss: 300.0841 - val_loss: 320.8060\nEpoch 54/100\n - 0s - loss: 289.3480 - val_loss: 310.5213\nEpoch 55/100\n - 0s - loss: 279.1928 - val_loss: 301.4202\nEpoch 56/100\n - 0s - loss: 269.6837 - val_loss: 293.8548\nEpoch 57/100\n - 0s - loss: 261.4394 - val_loss: 286.1454\nEpoch 58/100\n - 0s - loss: 253.7752 - val_loss: 279.2428\nEpoch 59/100\n - 1s - loss: 246.5580 - val_loss: 273.3648\nEpoch 60/100\n - 0s - loss: 240.2453 - val_loss: 268.0082\nEpoch 61/100\n - 0s - loss: 234.5737 - val_loss: 262.6816\nEpoch 62/100\n - 0s - loss: 229.2024 - val_loss: 257.9389\nEpoch 63/100\n - 0s - loss: 224.4088 - val_loss: 253.6927\nEpoch 64/100\n - 0s - loss: 220.0555 - val_loss: 249.7618\nEpoch 65/100\n - 0s - loss: 216.0810 - val_loss: 246.3687\nEpoch 66/100\n - 0s - loss: 212.5732 - val_loss: 242.9117\nEpoch 67/100\n - 0s - loss: 209.0247 - val_loss: 240.0607\nEpoch 68/100\n - 0s - loss: 206.0652 - val_loss: 237.3440\nEpoch 69/100\n - 0s - loss: 203.2228 - val_loss: 234.8997\nEpoch 70/100\n - 0s - loss: 200.7279 - val_loss: 232.4707\nEpoch 71/100\n - 1s - loss: 198.2874 - val_loss: 230.2859\nEpoch 72/100\n - 0s - loss: 196.1331 - val_loss: 228.1165\nEpoch 73/100\n - 0s - loss: 194.0004 - val_loss: 226.1598\nEpoch 74/100\n - 1s - loss: 192.0662 - val_loss: 224.1619\nEpoch 75/100\n - 0s - loss: 190.2020 - val_loss: 222.4778\nEpoch 76/100\n - 0s - loss: 188.5653 - val_loss: 220.7622\nEpoch 77/100\n - 0s - loss: 186.9872 - val_loss: 219.1566\nEpoch 78/100\n - 0s - loss: 185.4158 - val_loss: 217.6613\nEpoch 79/100\n - 0s - loss: 184.0015 - val_loss: 216.2461\nEpoch 80/100\n - 0s - loss: 182.7377 - val_loss: 214.9073\nEpoch 81/100\n - 0s - loss: 181.3182 - val_loss: 213.5146\nEpoch 82/100\n - 0s - loss: 180.0954 - val_loss: 212.1857\nEpoch 83/100\n - 0s - loss: 178.8642 - val_loss: 210.8730\nEpoch 84/100\n - 0s - loss: 177.7221 - val_loss: 209.6151\nEpoch 85/100\n - 0s - loss: 176.5879 - val_loss: 208.4495\nEpoch 86/100\n - 0s - loss: 175.5523 - val_loss: 207.1530\nEpoch 87/100\n - 0s - loss: 174.4355 - val_loss: 206.1107\nEpoch 88/100\n - 0s - loss: 173.3896 - val_loss: 205.0028\nEpoch 89/100\n - 0s - loss: 172.3333 - val_loss: 203.8627\nEpoch 90/100\n - 0s - loss: 171.3719 - val_loss: 202.7296\nEpoch 91/100\n - 0s - loss: 170.3690 - val_loss: 201.6957\nEpoch 92/100\n - 0s - loss: 169.4025 - val_loss: 200.6566\nEpoch 93/100\n - 0s - loss: 168.4830 - val_loss: 199.5644\nEpoch 94/100\n - 0s - loss: 167.5919 - val_loss: 198.5937\nEpoch 95/100\n - 0s - loss: 166.5795 - val_loss: 197.5056\nEpoch 96/100\n - 0s - loss: 165.7268 - val_loss: 196.5589\nEpoch 97/100\n - 0s - loss: 164.8704 - val_loss: 195.4155\nEpoch 98/100\n - 0s - loss: 163.9418 - val_loss: 194.4438\nEpoch 99/100\n - 0s - loss: 163.0371 - val_loss: 193.4085\nEpoch 100/100\n - 0s - loss: 162.2092 - val_loss: 192.4970\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Use .predict() method to get an array of new predicted values of 'strenght' using test set\ny_hat=model.predict(X_test_norm)\n\n# Create data frame for comparing true Strength and Predicted Strength\ndf=pd.DataFrame(y_hat, y_test)\ndf.reset_index(inplace=True)\ndf.columns=['Strength', 'Predicted Strength']\n\n#Select first row as sample to report\ny_testC=df.iloc[0,0]\ny_hatC=df.iloc[0,1]\n\ndf.head()",
            "execution_count": 899,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 899,
                    "data": {
                        "text/plain": "   Strength  Predicted Strength\n0     44.52           27.100958\n1     50.53           40.859787\n2     21.82           42.636662\n3     38.80           39.090347\n4     55.60           59.708954",
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Strength</th>\n      <th>Predicted Strength</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>44.52</td>\n      <td>27.100958</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>50.53</td>\n      <td>40.859787</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>21.82</td>\n      <td>42.636662</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>38.80</td>\n      <td>39.090347</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>55.60</td>\n      <td>59.708954</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Evaluate the model\nscores=model.evaluate(X_test_norm, y_test, verbose=0)\n\n# Use Scikit-learn to corroborate that our MSE value between true Strength and Predicted Strength is correct\nfrom sklearn.metrics import mean_squared_error\n\nMSE=mean_squared_error(y_test, y_hat)\nMSEC=MSE   # To report\n\nprint('We get the last Mean Squared Error value on test set at Epoch 100/100 such as: {},'.format(np.around(scores, decimals=4)))\nprint('and the same value: {} computed with Scikit-learn, and it has DECREASED respect to step B and step A.'.format(np.around(MSE, decimals=4)))",
            "execution_count": 900,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "We get the last Mean Squared Error value on test set at Epoch 100/100 such as: 192.497,\nand the same value: 192.497 computed with Scikit-learn, and it has DECREASED respect to step B and step A.\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Extract/Get out the MSE values of our History object as dictionary using .history attribute\ndictionary=error.history\n\n# Select each array of values in the dictionary\nloss_train=dictionary['loss']\nval_loss_test=dictionary['val_loss']\n\n# Create a Data Frame displaying the 50 MSE values at train and test set\nerror_values=pd.DataFrame(val_loss_test, loss_train)\nerror_values.reset_index(inplace=True)\nerror_values.columns=['loss (train set)','validated loss (test set)']\nerror_values.head(100)",
            "execution_count": 901,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 901,
                    "data": {
                        "text/plain": "    loss (train set)  validated loss (test set)\n0        1618.145725                1555.029743\n1        1605.097920                1542.732692\n2        1592.216662                1530.645569\n3        1579.553503                1518.636982\n4        1566.723007                1506.366063\n5        1553.432342                1493.652889\n6        1539.509176                1480.357290\n7        1524.862104                1466.136634\n8        1509.127612                1451.027713\n9        1492.496312                1434.757903\n10       1474.456890                1417.384985\n11       1454.996148                1398.487576\n12       1433.854028                1377.835044\n13       1411.046997                1355.640564\n14       1386.472367                1331.813948\n15       1360.243121                1305.991877\n16       1332.048733                1278.928680\n17       1302.680803                1250.599717\n18       1271.721930                1221.316877\n19       1239.785031                1190.376532\n20       1206.765735                1158.911026\n21       1173.236967                1126.816240\n22       1138.569038                1095.144698\n23       1104.270662                1061.396831\n24       1068.444011                1027.647513\n25       1032.558719                 993.839026\n26        996.910205                 958.594401\n27        959.870389                 924.702585\n28        923.665916                 889.558587\n29        886.524510                 855.952860\n..               ...                        ...\n70        198.287397                 230.285858\n71        196.133071                 228.116454\n72        194.000391                 226.159792\n73        192.066171                 224.161945\n74        190.201996                 222.477818\n75        188.565312                 220.762198\n76        186.987161                 219.156608\n77        185.415774                 217.661302\n78        184.001481                 216.246093\n79        182.737736                 214.907298\n80        181.318212                 213.514610\n81        180.095396                 212.185746\n82        178.864246                 210.872978\n83        177.722141                 209.615097\n84        176.587935                 208.449509\n85        175.552305                 207.152961\n86        174.435543                 206.110710\n87        173.389631                 205.002811\n88        172.333273                 203.862682\n89        171.371853                 202.729582\n90        170.369046                 201.695650\n91        169.402506                 200.656645\n92        168.482962                 199.564435\n93        167.591915                 198.593681\n94        166.579511                 197.505584\n95        165.726757                 196.558852\n96        164.870399                 195.415514\n97        163.941808                 194.443816\n98        163.037132                 193.408522\n99        162.209241                 192.497024\n\n[100 rows x 2 columns]",
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>loss (train set)</th>\n      <th>validated loss (test set)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1618.145725</td>\n      <td>1555.029743</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1605.097920</td>\n      <td>1542.732692</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1592.216662</td>\n      <td>1530.645569</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1579.553503</td>\n      <td>1518.636982</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1566.723007</td>\n      <td>1506.366063</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1553.432342</td>\n      <td>1493.652889</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>1539.509176</td>\n      <td>1480.357290</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>1524.862104</td>\n      <td>1466.136634</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>1509.127612</td>\n      <td>1451.027713</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>1492.496312</td>\n      <td>1434.757903</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>1474.456890</td>\n      <td>1417.384985</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>1454.996148</td>\n      <td>1398.487576</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>1433.854028</td>\n      <td>1377.835044</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>1411.046997</td>\n      <td>1355.640564</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>1386.472367</td>\n      <td>1331.813948</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>1360.243121</td>\n      <td>1305.991877</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>1332.048733</td>\n      <td>1278.928680</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>1302.680803</td>\n      <td>1250.599717</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>1271.721930</td>\n      <td>1221.316877</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>1239.785031</td>\n      <td>1190.376532</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>1206.765735</td>\n      <td>1158.911026</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>1173.236967</td>\n      <td>1126.816240</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>1138.569038</td>\n      <td>1095.144698</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>1104.270662</td>\n      <td>1061.396831</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>1068.444011</td>\n      <td>1027.647513</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>1032.558719</td>\n      <td>993.839026</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>996.910205</td>\n      <td>958.594401</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>959.870389</td>\n      <td>924.702585</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>923.665916</td>\n      <td>889.558587</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>886.524510</td>\n      <td>855.952860</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>70</th>\n      <td>198.287397</td>\n      <td>230.285858</td>\n    </tr>\n    <tr>\n      <th>71</th>\n      <td>196.133071</td>\n      <td>228.116454</td>\n    </tr>\n    <tr>\n      <th>72</th>\n      <td>194.000391</td>\n      <td>226.159792</td>\n    </tr>\n    <tr>\n      <th>73</th>\n      <td>192.066171</td>\n      <td>224.161945</td>\n    </tr>\n    <tr>\n      <th>74</th>\n      <td>190.201996</td>\n      <td>222.477818</td>\n    </tr>\n    <tr>\n      <th>75</th>\n      <td>188.565312</td>\n      <td>220.762198</td>\n    </tr>\n    <tr>\n      <th>76</th>\n      <td>186.987161</td>\n      <td>219.156608</td>\n    </tr>\n    <tr>\n      <th>77</th>\n      <td>185.415774</td>\n      <td>217.661302</td>\n    </tr>\n    <tr>\n      <th>78</th>\n      <td>184.001481</td>\n      <td>216.246093</td>\n    </tr>\n    <tr>\n      <th>79</th>\n      <td>182.737736</td>\n      <td>214.907298</td>\n    </tr>\n    <tr>\n      <th>80</th>\n      <td>181.318212</td>\n      <td>213.514610</td>\n    </tr>\n    <tr>\n      <th>81</th>\n      <td>180.095396</td>\n      <td>212.185746</td>\n    </tr>\n    <tr>\n      <th>82</th>\n      <td>178.864246</td>\n      <td>210.872978</td>\n    </tr>\n    <tr>\n      <th>83</th>\n      <td>177.722141</td>\n      <td>209.615097</td>\n    </tr>\n    <tr>\n      <th>84</th>\n      <td>176.587935</td>\n      <td>208.449509</td>\n    </tr>\n    <tr>\n      <th>85</th>\n      <td>175.552305</td>\n      <td>207.152961</td>\n    </tr>\n    <tr>\n      <th>86</th>\n      <td>174.435543</td>\n      <td>206.110710</td>\n    </tr>\n    <tr>\n      <th>87</th>\n      <td>173.389631</td>\n      <td>205.002811</td>\n    </tr>\n    <tr>\n      <th>88</th>\n      <td>172.333273</td>\n      <td>203.862682</td>\n    </tr>\n    <tr>\n      <th>89</th>\n      <td>171.371853</td>\n      <td>202.729582</td>\n    </tr>\n    <tr>\n      <th>90</th>\n      <td>170.369046</td>\n      <td>201.695650</td>\n    </tr>\n    <tr>\n      <th>91</th>\n      <td>169.402506</td>\n      <td>200.656645</td>\n    </tr>\n    <tr>\n      <th>92</th>\n      <td>168.482962</td>\n      <td>199.564435</td>\n    </tr>\n    <tr>\n      <th>93</th>\n      <td>167.591915</td>\n      <td>198.593681</td>\n    </tr>\n    <tr>\n      <th>94</th>\n      <td>166.579511</td>\n      <td>197.505584</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>165.726757</td>\n      <td>196.558852</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>164.870399</td>\n      <td>195.415514</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>163.941808</td>\n      <td>194.443816</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>163.037132</td>\n      <td>193.408522</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>162.209241</td>\n      <td>192.497024</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows \u00d7 2 columns</p>\n</div>"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Report the mean and the standard deviation of the mean squared errors\n# Get Statistics with .describe() method and select just mean and std in 'error_values' data frame\nstats=error_values.describe()\nmeanC=stats.iloc[1,1] # To report\nstats[1:3]",
            "execution_count": 902,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 902,
                    "data": {
                        "text/plain": "      loss (train set)  validated loss (test set)\nmean        611.933344                 610.273983\nstd         509.011003                 473.208693",
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>loss (train set)</th>\n      <th>validated loss (test set)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>mean</th>\n      <td>611.933344</td>\n      <td>610.273983</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>509.011003</td>\n      <td>473.208693</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "#### The *mean* of Mean Squared Error on step C is lower than step B and step A"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "<a id='itemD'></a>\n### D. Increase the number of hidden layers\n#### Repeat part B but use a neural network with *Three hidden layers*, each of 10 nodes and ReLU activation function."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Define regression model function\ndef regression_model():\n    # Create model\n    model=Sequential()\n    \n    # Add First hidden layer with 10 nodes/neurons and ReLU activation function\n    model.add(Dense(10, activation='relu', input_shape=(n_cols,)))\n    \n    # Add Second hidden layer with 10 nodes/neurons and ReLU activation function \n    model.add(Dense(10, activation='relu'))\n    \n    # Add Third hidden layer with 10 nodes/neurons and ReLU activation function \n    model.add(Dense(10, activation='relu'))\n    \n    # Add Output layer with 1 node/neuron\n    model.add(Dense(1))\n    \n    # Compile model adding adam Optimizer and the mean squared error as the loss function.\n    model.compile(optimizer='adam', loss='mean_squared_error')\n    return model",
            "execution_count": 903,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Build our model\nmodel=regression_model()\n\n# Train/fit the model on the training set using 50 epochs, and validate the model on test set.\nerror=model.fit(X_train_norm, y_train, validation_data=(X_test_norm, y_test), epochs=50, verbose=2)",
            "execution_count": 904,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "Train on 721 samples, validate on 309 samples\nEpoch 1/50\n - 9s - loss: 1554.4776 - val_loss: 1486.8938\nEpoch 2/50\n - 0s - loss: 1526.6335 - val_loss: 1454.4655\nEpoch 3/50\n - 0s - loss: 1484.7901 - val_loss: 1404.2561\nEpoch 4/50\n - 0s - loss: 1420.6499 - val_loss: 1327.5391\nEpoch 5/50\n - 0s - loss: 1322.0023 - val_loss: 1213.8024\nEpoch 6/50\n - 0s - loss: 1178.0353 - val_loss: 1047.8646\nEpoch 7/50\n - 0s - loss: 979.5445 - val_loss: 837.1843\nEpoch 8/50\n - 0s - loss: 735.9884 - val_loss: 598.4784\nEpoch 9/50\n - 0s - loss: 490.9768 - val_loss: 408.5086\nEpoch 10/50\n - 0s - loss: 327.6814 - val_loss: 315.5331\nEpoch 11/50\n - 1s - loss: 257.2625 - val_loss: 282.8456\nEpoch 12/50\n - 1s - loss: 232.7503 - val_loss: 264.0977\nEpoch 13/50\n - 0s - loss: 217.3433 - val_loss: 248.8995\nEpoch 14/50\n - 0s - loss: 206.1156 - val_loss: 237.6190\nEpoch 15/50\n - 0s - loss: 198.1912 - val_loss: 228.5942\nEpoch 16/50\n - 0s - loss: 190.5329 - val_loss: 220.1448\nEpoch 17/50\n - 0s - loss: 184.2429 - val_loss: 214.3927\nEpoch 18/50\n - 0s - loss: 179.3201 - val_loss: 208.2530\nEpoch 19/50\n - 0s - loss: 174.4048 - val_loss: 204.0706\nEpoch 20/50\n - 0s - loss: 170.5544 - val_loss: 199.3454\nEpoch 21/50\n - 1s - loss: 166.8241 - val_loss: 195.4052\nEpoch 22/50\n - 0s - loss: 163.6465 - val_loss: 191.1968\nEpoch 23/50\n - 0s - loss: 160.7856 - val_loss: 187.8419\nEpoch 24/50\n - 0s - loss: 157.9325 - val_loss: 186.3198\nEpoch 25/50\n - 0s - loss: 155.4900 - val_loss: 182.2206\nEpoch 26/50\n - 0s - loss: 152.9034 - val_loss: 180.3406\nEpoch 27/50\n - 0s - loss: 150.6578 - val_loss: 177.5482\nEpoch 28/50\n - 0s - loss: 148.4379 - val_loss: 176.1768\nEpoch 29/50\n - 0s - loss: 146.4648 - val_loss: 173.8190\nEpoch 30/50\n - 0s - loss: 145.4438 - val_loss: 171.3952\nEpoch 31/50\n - 0s - loss: 143.6144 - val_loss: 170.4543\nEpoch 32/50\n - 0s - loss: 141.2550 - val_loss: 168.1174\nEpoch 33/50\n - 0s - loss: 139.5708 - val_loss: 165.7231\nEpoch 34/50\n - 0s - loss: 138.5987 - val_loss: 164.8316\nEpoch 35/50\n - 0s - loss: 137.2582 - val_loss: 163.1539\nEpoch 36/50\n - 0s - loss: 135.4739 - val_loss: 161.8836\nEpoch 37/50\n - 0s - loss: 134.1383 - val_loss: 161.1419\nEpoch 38/50\n - 1s - loss: 132.9770 - val_loss: 158.9439\nEpoch 39/50\n - 1s - loss: 131.7966 - val_loss: 157.5528\nEpoch 40/50\n - 0s - loss: 130.6187 - val_loss: 156.9467\nEpoch 41/50\n - 0s - loss: 129.5743 - val_loss: 155.5567\nEpoch 42/50\n - 0s - loss: 128.5670 - val_loss: 153.6715\nEpoch 43/50\n - 0s - loss: 127.3702 - val_loss: 152.9588\nEpoch 44/50\n - 0s - loss: 126.4183 - val_loss: 152.1579\nEpoch 45/50\n - 0s - loss: 126.2101 - val_loss: 150.5566\nEpoch 46/50\n - 0s - loss: 124.6359 - val_loss: 150.9031\nEpoch 47/50\n - 0s - loss: 123.7396 - val_loss: 148.7372\nEpoch 48/50\n - 0s - loss: 123.0022 - val_loss: 148.2802\nEpoch 49/50\n - 0s - loss: 121.9762 - val_loss: 146.6442\nEpoch 50/50\n - 0s - loss: 121.5029 - val_loss: 147.2921\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Use .predict() method to get an array of new predicted values of 'strenght' using test set\ny_hat=model.predict(X_test_norm)\n\n# Create data frame for comparing true Strength and Predicted Strength\ndf=pd.DataFrame(y_hat, y_test)\ndf.reset_index(inplace=True)\ndf.columns=['Strength', 'Predicted Strength']\n\n#Select first row as sample to report\ny_testD=df.iloc[0,0]\ny_hatD=df.iloc[0,1]\n\ndf.head()",
            "execution_count": 905,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 905,
                    "data": {
                        "text/plain": "   Strength  Predicted Strength\n0     44.52           34.165421\n1     50.53           41.173103\n2     21.82           38.667336\n3     38.80           37.060841\n4     55.60           62.264595",
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Strength</th>\n      <th>Predicted Strength</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>44.52</td>\n      <td>34.165421</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>50.53</td>\n      <td>41.173103</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>21.82</td>\n      <td>38.667336</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>38.80</td>\n      <td>37.060841</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>55.60</td>\n      <td>62.264595</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Evaluate the model\nscores=model.evaluate(X_test_norm, y_test, verbose=0)\n\n# Use Scikit-learn to corroborate that our MSE value between true Strength and Predicted Strength is correct\nfrom sklearn.metrics import mean_squared_error\n\nMSE=mean_squared_error(y_test, y_hat)\nMSED=MSE   # To report\n\nprint('We get the last Mean Squared Error value on test set at Epoch 50/50 such as: {},'.format(np.around(scores, decimals=4)))\nprint('and the same value: {} computed with Scikit-learn, and it has DECREASED respect to steps C, B and A.'.format(np.around(MSE, decimals=4)))",
            "execution_count": 906,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "We get the last Mean Squared Error value on test set at Epoch 50/50 such as: 147.2921,\nand the same value: 147.2921 computed with Scikit-learn, and it has DECREASED respect to steps C, B and A.\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Extract/Get out the MSE values of our History object as dictionary using .history attribute\ndictionary=error.history\n\n# Select each array of values in the dictionary\nloss_train=dictionary['loss']\nval_loss_test=dictionary['val_loss']\n\n# Create a Data Frame displaying the 50 MSE values at train and test set\nerror_values=pd.DataFrame(val_loss_test, loss_train)\nerror_values.reset_index(inplace=True)\nerror_values.columns=['loss (train set)','validated loss (test set)']\nerror_values.head(50)",
            "execution_count": 907,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 907,
                    "data": {
                        "text/plain": "    loss (train set)  validated loss (test set)\n0        1554.477566                1486.893808\n1        1526.633450                1454.465529\n2        1484.790070                1404.256083\n3        1420.649896                1327.539104\n4        1322.002280                1213.802449\n5        1178.035259                1047.864586\n6         979.544489                 837.184251\n7         735.988383                 598.478356\n8         490.976783                 408.508594\n9         327.681398                 315.533127\n10        257.262537                 282.845604\n11        232.750284                 264.097651\n12        217.343309                 248.899540\n13        206.115562                 237.619025\n14        198.191179                 228.594238\n15        190.532887                 220.144820\n16        184.242900                 214.392681\n17        179.320050                 208.253015\n18        174.404807                 204.070638\n19        170.554382                 199.345390\n20        166.824108                 195.405159\n21        163.646526                 191.196801\n22        160.785570                 187.841889\n23        157.932514                 186.319829\n24        155.490045                 182.220632\n25        152.903417                 180.340570\n26        150.657843                 177.548184\n27        148.437855                 176.176757\n28        146.464774                 173.819045\n29        145.443847                 171.395247\n30        143.614388                 170.454289\n31        141.254953                 168.117441\n32        139.570811                 165.723149\n33        138.598744                 164.831627\n34        137.258223                 163.153946\n35        135.473882                 161.883574\n36        134.138259                 161.141907\n37        132.977041                 158.943934\n38        131.796556                 157.552752\n39        130.618736                 156.946691\n40        129.574326                 155.556665\n41        128.566950                 153.671455\n42        127.370196                 152.958844\n43        126.418294                 152.157940\n44        126.210057                 150.556645\n45        124.635875                 150.903135\n46        123.739593                 148.737169\n47        123.002239                 148.280197\n48        121.976242                 146.644214\n49        121.502872                 147.292058",
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>loss (train set)</th>\n      <th>validated loss (test set)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1554.477566</td>\n      <td>1486.893808</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1526.633450</td>\n      <td>1454.465529</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1484.790070</td>\n      <td>1404.256083</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1420.649896</td>\n      <td>1327.539104</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1322.002280</td>\n      <td>1213.802449</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1178.035259</td>\n      <td>1047.864586</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>979.544489</td>\n      <td>837.184251</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>735.988383</td>\n      <td>598.478356</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>490.976783</td>\n      <td>408.508594</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>327.681398</td>\n      <td>315.533127</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>257.262537</td>\n      <td>282.845604</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>232.750284</td>\n      <td>264.097651</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>217.343309</td>\n      <td>248.899540</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>206.115562</td>\n      <td>237.619025</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>198.191179</td>\n      <td>228.594238</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>190.532887</td>\n      <td>220.144820</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>184.242900</td>\n      <td>214.392681</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>179.320050</td>\n      <td>208.253015</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>174.404807</td>\n      <td>204.070638</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>170.554382</td>\n      <td>199.345390</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>166.824108</td>\n      <td>195.405159</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>163.646526</td>\n      <td>191.196801</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>160.785570</td>\n      <td>187.841889</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>157.932514</td>\n      <td>186.319829</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>155.490045</td>\n      <td>182.220632</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>152.903417</td>\n      <td>180.340570</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>150.657843</td>\n      <td>177.548184</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>148.437855</td>\n      <td>176.176757</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>146.464774</td>\n      <td>173.819045</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>145.443847</td>\n      <td>171.395247</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>143.614388</td>\n      <td>170.454289</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>141.254953</td>\n      <td>168.117441</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>139.570811</td>\n      <td>165.723149</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>138.598744</td>\n      <td>164.831627</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>137.258223</td>\n      <td>163.153946</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>135.473882</td>\n      <td>161.883574</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>134.138259</td>\n      <td>161.141907</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>132.977041</td>\n      <td>158.943934</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>131.796556</td>\n      <td>157.552752</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>130.618736</td>\n      <td>156.946691</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>129.574326</td>\n      <td>155.556665</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>128.566950</td>\n      <td>153.671455</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>127.370196</td>\n      <td>152.958844</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>126.418294</td>\n      <td>152.157940</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>126.210057</td>\n      <td>150.556645</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>124.635875</td>\n      <td>150.903135</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>123.739593</td>\n      <td>148.737169</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>123.002239</td>\n      <td>148.280197</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>121.976242</td>\n      <td>146.644214</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>121.502872</td>\n      <td>147.292058</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Report the mean and the standard deviation of the mean squared errors\n# Get Statistics with .describe() method and select just mean and std in 'error_values' data frame\nstats=error_values.describe()\nmeanD=stats.iloc[1,1] # To report\nstats[1:3]",
            "execution_count": 908,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 908,
                    "data": {
                        "text/plain": "      loss (train set)  validated loss (test set)\nmean        343.967644                 347.211205\nstd         429.390928                 385.832155",
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>loss (train set)</th>\n      <th>validated loss (test set)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>mean</th>\n      <td>343.967644</td>\n      <td>347.211205</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>429.390928</td>\n      <td>385.832155</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "#### The *mean* of Mean Squared Error on step D is lower than steps C, B and A."
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "<a id='itemE'></a>\n## E. Additional\n### Use (3) Hidden Layers and 100 Epochs\n#### Repeat part C but use a neural network with *Three hidden layers*, each of 10 nodes, *ReLU* activation function and 100 *Epochs*."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Build our model\nmodel=regression_model()\n\n# Train/fit the model on the training set using 50 epochs, and validate the model on test set.\nerror=model.fit(X_train_norm, y_train, validation_data=(X_test_norm, y_test), epochs=100, verbose=2)",
            "execution_count": 909,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "Train on 721 samples, validate on 309 samples\nEpoch 1/100\n - 9s - loss: 1564.7969 - val_loss: 1494.7517\nEpoch 2/100\n - 0s - loss: 1533.1168 - val_loss: 1458.0330\nEpoch 3/100\n - 0s - loss: 1489.2541 - val_loss: 1408.4382\nEpoch 4/100\n - 0s - loss: 1427.2655 - val_loss: 1334.4746\nEpoch 5/100\n - 0s - loss: 1333.4475 - val_loss: 1218.7755\nEpoch 6/100\n - 0s - loss: 1185.1941 - val_loss: 1048.1217\nEpoch 7/100\n - 0s - loss: 980.8907 - val_loss: 827.3225\nEpoch 8/100\n - 0s - loss: 731.8673 - val_loss: 594.7780\nEpoch 9/100\n - 0s - loss: 499.3447 - val_loss: 407.1117\nEpoch 10/100\n - 0s - loss: 339.7944 - val_loss: 313.9081\nEpoch 11/100\n - 0s - loss: 268.4868 - val_loss: 277.2577\nEpoch 12/100\n - 0s - loss: 235.1848 - val_loss: 254.6318\nEpoch 13/100\n - 1s - loss: 215.5390 - val_loss: 239.6493\nEpoch 14/100\n - 1s - loss: 203.0392 - val_loss: 227.5382\nEpoch 15/100\n - 0s - loss: 193.2609 - val_loss: 219.1504\nEpoch 16/100\n - 0s - loss: 185.7954 - val_loss: 211.6006\nEpoch 17/100\n - 0s - loss: 180.5979 - val_loss: 205.2527\nEpoch 18/100\n - 0s - loss: 175.6389 - val_loss: 200.8293\nEpoch 19/100\n - 0s - loss: 171.7137 - val_loss: 195.2598\nEpoch 20/100\n - 0s - loss: 168.1919 - val_loss: 192.0834\nEpoch 21/100\n - 0s - loss: 164.9490 - val_loss: 188.3838\nEpoch 22/100\n - 0s - loss: 162.1620 - val_loss: 184.5427\nEpoch 23/100\n - 0s - loss: 159.5276 - val_loss: 181.7941\nEpoch 24/100\n - 0s - loss: 157.2414 - val_loss: 179.2346\nEpoch 25/100\n - 0s - loss: 154.9689 - val_loss: 176.9324\nEpoch 26/100\n - 0s - loss: 152.6569 - val_loss: 173.6030\nEpoch 27/100\n - 0s - loss: 150.5969 - val_loss: 171.5158\nEpoch 28/100\n - 0s - loss: 148.3544 - val_loss: 169.4802\nEpoch 29/100\n - 0s - loss: 146.4936 - val_loss: 167.6110\nEpoch 30/100\n - 0s - loss: 144.6729 - val_loss: 165.0254\nEpoch 31/100\n - 0s - loss: 142.8100 - val_loss: 163.8025\nEpoch 32/100\n - 0s - loss: 141.0468 - val_loss: 161.4630\nEpoch 33/100\n - 0s - loss: 139.6304 - val_loss: 159.6950\nEpoch 34/100\n - 0s - loss: 137.1663 - val_loss: 157.5194\nEpoch 35/100\n - 0s - loss: 135.3224 - val_loss: 156.4354\nEpoch 36/100\n - 0s - loss: 133.6914 - val_loss: 154.8264\nEpoch 37/100\n - 0s - loss: 132.4556 - val_loss: 152.2150\nEpoch 38/100\n - 0s - loss: 130.5121 - val_loss: 151.0271\nEpoch 39/100\n - 0s - loss: 128.8354 - val_loss: 149.0670\nEpoch 40/100\n - 0s - loss: 127.3050 - val_loss: 148.0362\nEpoch 41/100\n - 1s - loss: 125.8934 - val_loss: 146.5059\nEpoch 42/100\n - 1s - loss: 124.6087 - val_loss: 145.7036\nEpoch 43/100\n - 0s - loss: 123.3483 - val_loss: 143.7929\nEpoch 44/100\n - 0s - loss: 121.8942 - val_loss: 143.0394\nEpoch 45/100\n - 0s - loss: 120.8034 - val_loss: 142.3421\nEpoch 46/100\n - 0s - loss: 119.8086 - val_loss: 140.1143\nEpoch 47/100\n - 0s - loss: 118.3708 - val_loss: 139.8726\nEpoch 48/100\n - 0s - loss: 116.8462 - val_loss: 137.9433\nEpoch 49/100\n - 0s - loss: 115.5564 - val_loss: 136.8435\nEpoch 50/100\n - 0s - loss: 114.7544 - val_loss: 135.7437\nEpoch 51/100\n - 0s - loss: 113.7776 - val_loss: 135.5394\nEpoch 52/100\n - 0s - loss: 112.9915 - val_loss: 133.8266\nEpoch 53/100\n - 0s - loss: 111.6038 - val_loss: 133.9432\nEpoch 54/100\n - 1s - loss: 110.6556 - val_loss: 132.8631\nEpoch 55/100\n - 0s - loss: 109.8593 - val_loss: 131.5697\nEpoch 56/100\n - 0s - loss: 108.7271 - val_loss: 131.2155\nEpoch 57/100\n - 0s - loss: 107.7873 - val_loss: 130.9408\nEpoch 58/100\n - 0s - loss: 106.8988 - val_loss: 129.7728\nEpoch 59/100\n - 0s - loss: 106.2525 - val_loss: 128.6567\nEpoch 60/100\n - 0s - loss: 105.1832 - val_loss: 129.2292\nEpoch 61/100\n - 0s - loss: 104.6367 - val_loss: 127.5925\nEpoch 62/100\n - 0s - loss: 104.0972 - val_loss: 126.9978\nEpoch 63/100\n - 0s - loss: 103.2681 - val_loss: 126.2081\nEpoch 64/100\n - 0s - loss: 102.4226 - val_loss: 125.6845\nEpoch 65/100\n - 0s - loss: 101.9644 - val_loss: 123.7171\nEpoch 66/100\n - 0s - loss: 101.3334 - val_loss: 124.2906\nEpoch 67/100\n - 1s - loss: 100.3310 - val_loss: 122.9175\nEpoch 68/100\n - 1s - loss: 100.0159 - val_loss: 122.7031\nEpoch 69/100\n - 0s - loss: 98.8938 - val_loss: 122.3586\nEpoch 70/100\n - 0s - loss: 98.1715 - val_loss: 121.2556\nEpoch 71/100\n - 0s - loss: 98.0517 - val_loss: 120.6252\nEpoch 72/100\n - 0s - loss: 97.7376 - val_loss: 120.7705\nEpoch 73/100\n - 0s - loss: 96.2975 - val_loss: 119.6688\nEpoch 74/100\n - 0s - loss: 95.6964 - val_loss: 118.1269\nEpoch 75/100\n - 0s - loss: 95.1457 - val_loss: 117.8795\nEpoch 76/100\n - 0s - loss: 94.6914 - val_loss: 117.4562\nEpoch 77/100\n - 0s - loss: 93.8686 - val_loss: 116.0699\nEpoch 78/100\n - 0s - loss: 93.8761 - val_loss: 115.2730\nEpoch 79/100\n - 1s - loss: 92.5023 - val_loss: 114.3013\nEpoch 80/100\n - 0s - loss: 91.5118 - val_loss: 114.0835\nEpoch 81/100\n - 0s - loss: 90.8539 - val_loss: 112.9975\nEpoch 82/100\n - 0s - loss: 90.3668 - val_loss: 112.3865\nEpoch 83/100\n - 0s - loss: 89.3734 - val_loss: 111.0524\nEpoch 84/100\n - 0s - loss: 88.6149 - val_loss: 110.4106\nEpoch 85/100\n - 0s - loss: 88.2239 - val_loss: 110.2109\nEpoch 86/100\n - 0s - loss: 87.5458 - val_loss: 108.4627\nEpoch 87/100\n - 1s - loss: 86.7301 - val_loss: 107.0349\nEpoch 88/100\n - 0s - loss: 86.0415 - val_loss: 106.5519\nEpoch 89/100\n - 0s - loss: 85.4663 - val_loss: 105.5570\nEpoch 90/100\n - 0s - loss: 84.8891 - val_loss: 105.7354\nEpoch 91/100\n - 0s - loss: 84.0863 - val_loss: 104.8027\nEpoch 92/100\n - 0s - loss: 83.5532 - val_loss: 103.7659\nEpoch 93/100\n - 1s - loss: 83.1321 - val_loss: 103.6318\nEpoch 94/100\n - 1s - loss: 82.3071 - val_loss: 102.8080\nEpoch 95/100\n - 0s - loss: 81.7130 - val_loss: 101.3535\nEpoch 96/100\n - 0s - loss: 81.2709 - val_loss: 101.4696\nEpoch 97/100\n - 0s - loss: 80.5976 - val_loss: 100.2498\nEpoch 98/100\n - 0s - loss: 79.8685 - val_loss: 100.6333\nEpoch 99/100\n - 0s - loss: 79.3507 - val_loss: 97.7766\nEpoch 100/100\n - 0s - loss: 78.8951 - val_loss: 99.0254\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Use .predict() method to get an array of new predicted values of 'strenght' using test set\ny_hat=model.predict(X_test_norm)\n\n# Create data frame for comparing true Strength and Predicted Strength\ndf=pd.DataFrame(y_hat, y_test)\ndf.reset_index(inplace=True)\ndf.columns=['Strength', 'Predicted Strength']\n\n#Select first row as sample to report\ny_testE=df.iloc[0,0]\ny_hatE=df.iloc[0,1]\n\ndf.head()",
            "execution_count": 910,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 910,
                    "data": {
                        "text/plain": "   Strength  Predicted Strength\n0     44.52           42.489040\n1     50.53           49.544811\n2     21.82           32.233387\n3     38.80           35.205288\n4     55.60           60.650658",
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Strength</th>\n      <th>Predicted Strength</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>44.52</td>\n      <td>42.489040</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>50.53</td>\n      <td>49.544811</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>21.82</td>\n      <td>32.233387</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>38.80</td>\n      <td>35.205288</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>55.60</td>\n      <td>60.650658</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Evaluate the model\nscores=model.evaluate(X_test_norm, y_test, verbose=0)\n\n# Use Scikit-learn to corroborate that our MSE value between true Strength and Predicted Strength is correct\nfrom sklearn.metrics import mean_squared_error\n\nMSE=mean_squared_error(y_test, y_hat)\nMSEE=MSE   # To report\n\nprint('We get the last Mean Squared Error value on test set at Epoch 100/100 such as: {},'.format(np.around(scores, decimals=4)))\nprint('and the same value: {} computed with Scikit-learn, and it has DECREASED respect to steps D, C, B and A.'.format(np.around(MSE, decimals=4)))",
            "execution_count": 911,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "We get the last Mean Squared Error value on test set at Epoch 100/100 such as: 99.0254,\nand the same value: 99.0254 computed with Scikit-learn, and it has DECREASED respect to steps D, C, B and A.\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Extract/Get out the MSE values of our History object as dictionary using .history attribute\ndictionary=error.history\n\n# Select each array of values in the dictionary\nloss_train=dictionary['loss']\nval_loss_test=dictionary['val_loss']\n\n# Create a Data Frame displaying the 50 MSE values at train and test set\nerror_values=pd.DataFrame(val_loss_test, loss_train)\nerror_values.reset_index(inplace=True)\nerror_values.columns=['loss (train set)','validated loss (test set)']\nerror_values.head(100)",
            "execution_count": 912,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 912,
                    "data": {
                        "text/plain": "    loss (train set)  validated loss (test set)\n0        1564.796854                1494.751742\n1        1533.116815                1458.033021\n2        1489.254119                1408.438209\n3        1427.265465                1334.474612\n4        1333.447491                1218.775482\n5        1185.194066                1048.121675\n6         980.890710                 827.322451\n7         731.867312                 594.778048\n8         499.344653                 407.111743\n9         339.794368                 313.908078\n10        268.486788                 277.257651\n11        235.184819                 254.631813\n12        215.538956                 239.649252\n13        203.039226                 227.538214\n14        193.260915                 219.150378\n15        185.795412                 211.600640\n16        180.597924                 205.252685\n17        175.638943                 200.829250\n18        171.713663                 195.259847\n19        168.191925                 192.083379\n20        164.949023                 188.383798\n21        162.161953                 184.542744\n22        159.527570                 181.794104\n23        157.241389                 179.234612\n24        154.968926                 176.932432\n25        152.656927                 173.602979\n26        150.596893                 171.515829\n27        148.354391                 169.480170\n28        146.493611                 167.611013\n29        144.672925                 165.025369\n..               ...                        ...\n70         98.051671                 120.625196\n71         97.737604                 120.770501\n72         96.297509                 119.668791\n73         95.696422                 118.126918\n74         95.145662                 117.879475\n75         94.691402                 117.456174\n76         93.868630                 116.069895\n77         93.876089                 115.273032\n78         92.502278                 114.301328\n79         91.511837                 114.083460\n80         90.853859                 112.997486\n81         90.366753                 112.386520\n82         89.373396                 111.052426\n83         88.614931                 110.410596\n84         88.223900                 110.210896\n85         87.545771                 108.462737\n86         86.730099                 107.034882\n87         86.041469                 106.551869\n88         85.466309                 105.556976\n89         84.889111                 105.735401\n90         84.086313                 104.802725\n91         83.553157                 103.765928\n92         83.132128                 103.631767\n93         82.307074                 102.808030\n94         81.712962                 101.353529\n95         81.270925                 101.469554\n96         80.597643                 100.249753\n97         79.868472                 100.633336\n98         79.350741                  97.776585\n99         78.895097                  99.025403\n\n[100 rows x 2 columns]",
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>loss (train set)</th>\n      <th>validated loss (test set)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1564.796854</td>\n      <td>1494.751742</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1533.116815</td>\n      <td>1458.033021</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1489.254119</td>\n      <td>1408.438209</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1427.265465</td>\n      <td>1334.474612</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1333.447491</td>\n      <td>1218.775482</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1185.194066</td>\n      <td>1048.121675</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>980.890710</td>\n      <td>827.322451</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>731.867312</td>\n      <td>594.778048</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>499.344653</td>\n      <td>407.111743</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>339.794368</td>\n      <td>313.908078</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>268.486788</td>\n      <td>277.257651</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>235.184819</td>\n      <td>254.631813</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>215.538956</td>\n      <td>239.649252</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>203.039226</td>\n      <td>227.538214</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>193.260915</td>\n      <td>219.150378</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>185.795412</td>\n      <td>211.600640</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>180.597924</td>\n      <td>205.252685</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>175.638943</td>\n      <td>200.829250</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>171.713663</td>\n      <td>195.259847</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>168.191925</td>\n      <td>192.083379</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>164.949023</td>\n      <td>188.383798</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>162.161953</td>\n      <td>184.542744</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>159.527570</td>\n      <td>181.794104</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>157.241389</td>\n      <td>179.234612</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>154.968926</td>\n      <td>176.932432</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>152.656927</td>\n      <td>173.602979</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>150.596893</td>\n      <td>171.515829</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>148.354391</td>\n      <td>169.480170</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>146.493611</td>\n      <td>167.611013</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>144.672925</td>\n      <td>165.025369</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>70</th>\n      <td>98.051671</td>\n      <td>120.625196</td>\n    </tr>\n    <tr>\n      <th>71</th>\n      <td>97.737604</td>\n      <td>120.770501</td>\n    </tr>\n    <tr>\n      <th>72</th>\n      <td>96.297509</td>\n      <td>119.668791</td>\n    </tr>\n    <tr>\n      <th>73</th>\n      <td>95.696422</td>\n      <td>118.126918</td>\n    </tr>\n    <tr>\n      <th>74</th>\n      <td>95.145662</td>\n      <td>117.879475</td>\n    </tr>\n    <tr>\n      <th>75</th>\n      <td>94.691402</td>\n      <td>117.456174</td>\n    </tr>\n    <tr>\n      <th>76</th>\n      <td>93.868630</td>\n      <td>116.069895</td>\n    </tr>\n    <tr>\n      <th>77</th>\n      <td>93.876089</td>\n      <td>115.273032</td>\n    </tr>\n    <tr>\n      <th>78</th>\n      <td>92.502278</td>\n      <td>114.301328</td>\n    </tr>\n    <tr>\n      <th>79</th>\n      <td>91.511837</td>\n      <td>114.083460</td>\n    </tr>\n    <tr>\n      <th>80</th>\n      <td>90.853859</td>\n      <td>112.997486</td>\n    </tr>\n    <tr>\n      <th>81</th>\n      <td>90.366753</td>\n      <td>112.386520</td>\n    </tr>\n    <tr>\n      <th>82</th>\n      <td>89.373396</td>\n      <td>111.052426</td>\n    </tr>\n    <tr>\n      <th>83</th>\n      <td>88.614931</td>\n      <td>110.410596</td>\n    </tr>\n    <tr>\n      <th>84</th>\n      <td>88.223900</td>\n      <td>110.210896</td>\n    </tr>\n    <tr>\n      <th>85</th>\n      <td>87.545771</td>\n      <td>108.462737</td>\n    </tr>\n    <tr>\n      <th>86</th>\n      <td>86.730099</td>\n      <td>107.034882</td>\n    </tr>\n    <tr>\n      <th>87</th>\n      <td>86.041469</td>\n      <td>106.551869</td>\n    </tr>\n    <tr>\n      <th>88</th>\n      <td>85.466309</td>\n      <td>105.556976</td>\n    </tr>\n    <tr>\n      <th>89</th>\n      <td>84.889111</td>\n      <td>105.735401</td>\n    </tr>\n    <tr>\n      <th>90</th>\n      <td>84.086313</td>\n      <td>104.802725</td>\n    </tr>\n    <tr>\n      <th>91</th>\n      <td>83.553157</td>\n      <td>103.765928</td>\n    </tr>\n    <tr>\n      <th>92</th>\n      <td>83.132128</td>\n      <td>103.631767</td>\n    </tr>\n    <tr>\n      <th>93</th>\n      <td>82.307074</td>\n      <td>102.808030</td>\n    </tr>\n    <tr>\n      <th>94</th>\n      <td>81.712962</td>\n      <td>101.353529</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>81.270925</td>\n      <td>101.469554</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>80.597643</td>\n      <td>100.249753</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>79.868472</td>\n      <td>100.633336</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>79.350741</td>\n      <td>97.776585</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>78.895097</td>\n      <td>99.025403</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows \u00d7 2 columns</p>\n</div>"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Report the mean and the standard deviation of the mean squared errors\n# Get Statistics with .describe() method and select just mean and std in 'error_values' data frame\nstats=error_values.describe()\nmeanE=stats.iloc[1,1] # To report\nstats[1:3]",
            "execution_count": 913,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 913,
                    "data": {
                        "text/plain": "      loss (train set)  validated loss (test set)\nmean        219.258341                 228.945331\nstd         329.086120                 296.614918",
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>loss (train set)</th>\n      <th>validated loss (test set)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>mean</th>\n      <td>219.258341</td>\n      <td>228.945331</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>329.086120</td>\n      <td>296.614918</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "#### The *mean* of Mean Squared Error on step E is lower than steps D, C, B and A."
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "<a id='itemF'></a>\n# F. Report"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "report_array=[\n             {'Step': 'A', 'Mean of MSE (Test Set)':meanA, 'True Strength':y_testA, 'Predicted Strength':y_hatA, 'MSE last Epoch (Test Set)':MSEA},\n             {'Step': 'B', 'Mean of MSE (Test Set)':meanB, 'True Strength':y_testB, 'Predicted Strength':y_hatB, 'MSE last Epoch (Test Set)':MSEB},\n             {'Step': 'C', 'Mean of MSE (Test Set)':meanC, 'True Strength':y_testC, 'Predicted Strength':y_hatC, 'MSE last Epoch (Test Set)':MSEC},\n             {'Step': 'D', 'Mean of MSE (Test Set)':meanD, 'True Strength':y_testD, 'Predicted Strength':y_hatD, 'MSE last Epoch (Test Set)':MSED},\n             {'Step': 'E', 'Mean of MSE (Test Set)':meanE, 'True Strength':y_testE, 'Predicted Strength':y_hatE, 'MSE last Epoch (Test Set)':MSEE}\n             ]\n\nreport_df=pd.DataFrame(report_array)\nreport_df",
            "execution_count": 914,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 914,
                    "data": {
                        "text/plain": "   MSE last Epoch (Test Set)  Mean of MSE (Test Set)  Predicted Strength Step  \\\n0                 137.699862             2322.370858           40.439449    A   \n1                 311.916119              931.623949           27.473602    B   \n2                 192.497021              610.273983           27.100958    C   \n3                 147.292055              347.211205           34.165421    D   \n4                  99.025402              228.945331           42.489040    E   \n\n   True Strength  \n0          44.52  \n1          44.52  \n2          44.52  \n3          44.52  \n4          44.52  ",
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MSE last Epoch (Test Set)</th>\n      <th>Mean of MSE (Test Set)</th>\n      <th>Predicted Strength</th>\n      <th>Step</th>\n      <th>True Strength</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>137.699862</td>\n      <td>2322.370858</td>\n      <td>40.439449</td>\n      <td>A</td>\n      <td>44.52</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>311.916119</td>\n      <td>931.623949</td>\n      <td>27.473602</td>\n      <td>B</td>\n      <td>44.52</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>192.497021</td>\n      <td>610.273983</td>\n      <td>27.100958</td>\n      <td>C</td>\n      <td>44.52</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>147.292055</td>\n      <td>347.211205</td>\n      <td>34.165421</td>\n      <td>D</td>\n      <td>44.52</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>99.025402</td>\n      <td>228.945331</td>\n      <td>42.489040</td>\n      <td>E</td>\n      <td>44.52</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "# <span style=\"color:gray\"><strong>Thanks for Watching !</strong></span>\n### <span style=\"color:gray\"><strong>Diego H. Salazar A.</strong></span>"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "<img src=\"https://media-exp1.licdn.com/dms/image/C5103AQH_4IJSGAl9Yw/profile-displayphoto-shrink_200_200/0?e=1602115200&v=beta&t=oi-1AzhyTRYSGTOqgeC27692BfY63DtzdqS9eHVZKPk\" width=\"60\" height=\"60\" align=\"left\"/>"
        }
    ],
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.6",
            "language": "python"
        },
        "language_info": {
            "name": "python",
            "version": "3.6.9",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}